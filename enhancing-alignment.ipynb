{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-01-31T18:26:05.933680Z","iopub.status.busy":"2025-01-31T18:26:05.933328Z","iopub.status.idle":"2025-01-31T18:26:11.523895Z","shell.execute_reply":"2025-01-31T18:26:11.522876Z","shell.execute_reply.started":"2025-01-31T18:26:05.933650Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformer_lens\n","  Downloading transformer_lens-2.11.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.2.1)\n","Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n","  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n","Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n","  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (3.2.0)\n","Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.8.0)\n","Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n","  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n","Collecting jaxtyping>=0.2.11 (from transformer_lens)\n","  Downloading jaxtyping-0.2.37-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.26.4)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.2.2)\n","Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.9.4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.0)\n","Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.5.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.67.1)\n","Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.47.0)\n","Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.4.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.12.2)\n","Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.19.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.27.0)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.11.10)\n","Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer_lens)\n","  Downloading wadler_lindig-0.1.3-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->transformer_lens) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->transformer_lens) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->transformer_lens) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->transformer_lens) (2025.0.1)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->transformer_lens) (2022.0.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->transformer_lens) (2.4.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10->transformer_lens) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (0.21.0)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (4.3.6)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n","Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (2.10.3)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (2.19.2)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (75.1.0)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.18.3)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (3.0.2)\n","Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->transformer_lens) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->transformer_lens) (2022.0.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->transformer_lens) (1.2.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->transformer_lens) (2024.2.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->transformer_lens) (2024.2.0)\n","Downloading transformer_lens-2.11.0-py3-none-any.whl (177 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading beartype-0.14.1-py3-none-any.whl (739 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n","Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n","Downloading jaxtyping-0.2.37-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wadler_lindig-0.1.3-py3-none-any.whl (20 kB)\n","Installing collected packages: better-abc, wadler-lindig, fancy-einsum, beartype, jaxtyping, transformer_lens\n","Successfully installed beartype-0.14.1 better-abc-0.0.3 fancy-einsum-0.0.3 jaxtyping-0.2.37 transformer_lens-2.11.0 wadler-lindig-0.1.3\n"]}],"source":["!pip install transformer_lens"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-01-31T18:26:11.525703Z","iopub.status.busy":"2025-01-31T18:26:11.525373Z","iopub.status.idle":"2025-01-31T18:26:34.657207Z","shell.execute_reply":"2025-01-31T18:26:34.656488Z","shell.execute_reply.started":"2025-01-31T18:26:11.525671Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import Tensor\n","import torch.nn.functional as F\n","import functools\n","import einops\n","import gc\n","from itertools import islice\n","from tqdm import tqdm\n","from typing import Callable, Dict, List, Set, Tuple\n","from transformer_lens import HookedTransformer, utils, ActivationCache\n","from transformer_lens.hook_points import HookPoint\n","from jaxtyping import Float, Int\n","from sklearn.model_selection import train_test_split\n","\n","class ChatTemplate:\n","    def __init__(self,model,template):\n","        self.model = model\n","        self.template = template\n","\n","    def format(self,instruction):\n","        return self.template.format(instruction=instruction)\n","\n","    def __enter__(self):\n","        self.prev = self.model.chat_template\n","        self.model.chat_template = self\n","        return self\n","\n","    def __exit__(self,exc,exc_value,exc_tb):\n","        self.model.chat_template = self.prev\n","        del self.prev\n","\n","\n","LLAMA3_CHAT_TEMPLATE = \"\"\"<|start_header_id|>user<|end_header_id|>\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\"\n","PHI3_CHAT_TEMPLATE = \"\"\"<|user|>\\n{instruction}<|end|>\\n<|assistant|>\"\"\"\n","class ReverseAbliterator:\n","    def __init__(\n","        self,\n","        model: str,\n","        dataset: Tuple[List[str], List[str]]|List[Tuple[List[str], List[str]]],\n","        device: str = 'cuda',\n","        n_devices: int = None,\n","        cache_fname: str = None,\n","        activation_layers: List[str] = ['resid_pre', 'resid_post', 'mlp_out', 'attn_out'],\n","        chat_template: str = None,\n","        target_toks: List[int]|Tuple[int]|Set[int]|Int[Tensor, '...'] = None,\n","    ):\n","        self.MODEL_PATH = model\n","        if n_devices is None and torch.cuda.is_available():\n","            n_devices = torch.cuda.device_count()\n","        elif n_devices is None:\n","            n_devices = 1\n","\n","        torch.set_grad_enabled(False)\n","\n","        self.model = HookedTransformer.from_pretrained_no_processing(\n","            model,\n","            n_devices=n_devices,\n","            device=device,\n","            dtype=torch.bfloat16,\n","            default_padding_side='left'\n","        )\n","\n","        self.model.requires_grad_(False)\n","\n","        self.model.tokenizer.padding_side = 'left'\n","        self.model.tokenizer.pad_token = self.model.tokenizer.eos_token\n","        self.chat_template = chat_template or ChatTemplate(self, LLAMA3_CHAT_TEMPLATE)\n","\n","        self.hidden_size = self.model.cfg.d_model\n","        self.original_state = {k:v.to('cpu') for k,v in self.model.state_dict().items()}\n","        self.target = {}\n","        self.baseline = {}\n","        self.modified_layers = {'mlp':{}, 'W_O':{}}\n","        self.checkpoints = []\n","\n","        if cache_fname is not None:\n","            outs = torch.load(cache_fname, map_location='cpu')\n","            self.target, self.baseline, modified_layers, checkpoints = outs[:4]\n","            self.checkpoints = checkpoints or []\n","            self.modified_layers = modified_layers\n","\n","        self.target_inst_train, self.target_inst_test = prepare_dataset(dataset[0])\n","        self.baseline_inst_train, self.baseline_inst_test = prepare_dataset(dataset[1])\n","\n","        self.fwd_hooks = []\n","        self.modified = False\n","        self.activation_layers = [activation_layers] if isinstance(activation_layers, str) else activation_layers\n","        self.target_toks = target_toks or {32, 1271, 8586, 96556, 78145}  # Default to some positive tokens\n","        self._blacklisted = set()\n","\n","    def reset_state(self):\n","        self.modified = False\n","        self.modified_layers = {'mlp':{}, 'W_O':{}}\n","        self.model.load_state_dict(self.original_state)\n","\n","    def checkpoint(self):\n","        self.checkpoints.append(self.modified_layers.copy())\n","\n","    def save_activations(self, fname: str):\n","        torch.save([self.target, self.baseline, self.modified_layers if self.modified_layers['mlp'] or self.modified_layers['W_O'] else None, self.checkpoints if len(self.checkpoints) > 0 else None], fname)\n","\n","    def calculate_enhancement_dirs(self, key: str) -> Dict[str, Float[Tensor, 'd_model']]:\n","        dirs = {\n","            'target_mean': torch.mean(self.target[key], dim=0),\n","            'baseline_mean': torch.mean(self.baseline[key], dim=0)\n","        }\n","        dirs['enhancement_dir'] = dirs['target_mean'] - dirs['baseline_mean']\n","        return dirs\n","\n","    def enhancement_dirs(self) -> Dict[str, Float[Tensor, 'd_model']]:\n","        if not self.target:\n","            raise IndexError(\"No cache\")\n","\n","        enhancement_dirs = {key: self.calculate_enhancement_dirs(key) for key in self.target if '.0.' not in key}\n","        return {key: (v['enhancement_dir'] / v['enhancement_dir'].norm()).to('cpu') for key, v in enhancement_dirs.items()}\n","\n","    def apply_enhancement_dirs(\n","        self,\n","        enhancement_dirs: List[Float[Tensor, 'd_model']],\n","        W_O: bool = True,\n","        mlp: bool = True,\n","        layers: List[int] = None,\n","        strength: float = 1.0\n","    ):\n","        if layers is None:\n","            layers = list(range(1, self.model.cfg.n_layers))\n","        for enhancement_dir in enhancement_dirs:\n","            for layer in layers:\n","                for modifying in [(W_O, self.layer_attn), (mlp, self.layer_mlp)]:\n","                    if modifying[0]:\n","                        matrix = modifying[1](layer)\n","                        if enhancement_dir.device != matrix.device:\n","                            enhancement_dir = enhancement_dir.to(matrix.device)\n","                        proj = einops.einsum(matrix, enhancement_dir.view(-1, 1), '... d_model, d_model single -> ... single') * enhancement_dir\n","                        modifying[1](layer, matrix + strength * proj)\n","\n","    def layer_attn(self, layer: int, replacement: Float[Tensor, \"d_model\"] = None) -> Float[Tensor, \"d_model\"]:\n","        if replacement is not None and layer not in self._blacklisted:\n","            self.modified = True\n","            self.model.blocks[layer].attn.W_O.data = replacement.to(self.model.blocks[layer].attn.W_O.device)\n","            self.modified_layers['W_O'][layer] = self.modified_layers.get(layer, []) + [(self.model.blocks[layer].attn.W_O.data.to('cpu'), replacement.to('cpu'))]\n","        return self.model.blocks[layer].attn.W_O.data\n","\n","    def layer_mlp(self, layer: int, replacement: Float[Tensor, \"d_model\"] = None) -> Float[Tensor, \"d_model\"]:\n","        if replacement is not None and layer not in self._blacklisted:\n","            self.modified = True\n","            self.model.blocks[layer].mlp.W_out.data = replacement.to(self.model.blocks[layer].mlp.W_out.device)\n","            self.modified_layers['mlp'][layer] = self.modified_layers.get(layer, []) + [(self.model.blocks[layer].mlp.W_out.data.to('cpu'), replacement.to('cpu'))]\n","        return self.model.blocks[layer].mlp.W_out.data\n","\n","    def cache_activations(\n","        self,\n","        N: int = 128,\n","        batch_size: int = 8,\n","        last_indices: int = 1,\n","        reset: bool = True,\n","        activation_layers: int = -1,\n","        preserve_baseline: bool = True,\n","    ):\n","        if hasattr(self, \"current_state\"):\n","            print(\"WARNING: Caching activations using a context\")\n","        if self.modified:\n","            print(\"WARNING: Running modified model\")\n","\n","        if activation_layers == -1:\n","            activation_layers = self.activation_layers\n","\n","        baseline_is_set = len(getattr(self, \"baseline\", {})) > 0\n","        preserve_baseline = baseline_is_set and preserve_baseline\n","\n","        if reset or getattr(self, \"baseline\", None) is None:\n","            self.target = {}\n","            if not preserve_baseline:\n","                self.baseline = {}\n","\n","        toks = self.tokenize_instructions_fn(instructions=self.target_inst_train[:N] + self.baseline_inst_train[:N])\n","\n","        splitpos = min(N, len(self.target_inst_train))\n","        target_toks = toks[:splitpos]\n","        baseline_toks = toks[splitpos:]\n","\n","        last_indices = last_indices or 1\n","\n","        self.target = self.create_activation_cache(target_toks, N=N, batch_size=batch_size, last_indices=last_indices)\n","        if not preserve_baseline:\n","            self.baseline = self.create_activation_cache(baseline_toks, N=N, batch_size=batch_size, last_indices=last_indices)\n","\n","    def create_activation_cache(\n","        self,\n","        toks,\n","        N: int = 128,\n","        batch_size: int = 8,\n","        last_indices: int = 1,\n","    ) -> Dict[str, Float[Tensor, 'batch d_model']]:\n","        base = {}\n","        for i in tqdm(range(0, min(N, len(toks)), batch_size)):\n","            logits, cache = self.run_with_cache(toks[i:min(i+batch_size, len(toks))])\n","            for key in cache:\n","                if self.activation_layers is None or any(k in key for k in self.activation_layers):\n","                    tensor = torch.mean(cache[key][:, -last_indices:, :].to('cpu'), dim=1)\n","                    if key not in base:\n","                        base[key] = tensor\n","                    else:\n","                        base[key] = torch.cat((base[key], tensor), dim=0)\n","            del logits, cache\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        return base\n","\n","    def measure_enhancement(\n","        self,\n","        N: int = 4,\n","        sampled_token_ct: int = 8,\n","        measure: str = 'max',\n","    ) -> Dict[str, Float[Tensor, 'd_model']]:\n","        toks = self.tokenize_instructions_fn(instructions=self.target_inst_test[:N])\n","        logits, _ = self.run_with_cache(toks, max_new_tokens=sampled_token_ct)\n","\n","        enhancement_score = self.measure_enhancement_from_logits(logits, sampled_token_ct, measure=measure)\n","        return {'enhancement': enhancement_score.to('cpu')}\n","\n","    def measure_enhancement_from_logits(\n","        self,\n","        logits: Float[Tensor, 'batch_size seq_len d_vocab'],\n","        sequence: int,\n","        measure: str = 'max'\n","    ) -> Float[Tensor, 'batch_size']:\n","        normalized_scores = torch.softmax(logits[:, -sequence:, :].to('cpu'), dim=-1)[:, :, list(self.target_toks)]\n","        max_score_per_sequence = torch.max(normalized_scores, dim=-1)[0]\n","        score_per_batch = getattr(torch, measure)(max_score_per_sequence, dim=-1)[0]\n","        return score_per_batch\n","\n","    def run_with_cache(\n","        self,\n","        *model_args,\n","        names_filter: Callable[[str], bool] = None,\n","        max_new_tokens: int = 1,\n","        **model_kwargs\n","    ) -> Tuple[Float[Tensor, 'batch_size seq_len d_vocab'], Dict[str, Float[Tensor, 'batch_size seq_len d_model']]]:\n","        if names_filter is None and self.activation_layers:\n","            names_filter = lambda namefunc: any(s in namefunc for s in self.activation_layers)\n","\n","        # Modified line - removed pos_slice parameter\n","        cache_dict, fwd, _ = self.model.get_caching_hooks(\n","            names_filter,\n","            remove_batch_dim=False\n","        )\n","\n","        fwd_hooks = fwd + self.fwd_hooks\n","\n","        with self.model.hooks(fwd_hooks=fwd_hooks):\n","            model_out, _ = self.generate_logits(*model_args, max_tokens_generated=max_new_tokens, **model_kwargs)\n","\n","        return model_out, cache_dict\n","\n","    def generate_logits(\n","        self,\n","        toks: Int[Tensor, 'batch_size seq_len'],\n","        *args,\n","        max_tokens_generated: int = 1,\n","        **kwargs\n","    ) -> Tuple[Float[Tensor, 'batch_size seq_len d_vocab'], Int[Tensor, 'batch_size seq_len']]:\n","        all_toks = torch.zeros((toks.shape[0], toks.shape[1] + max_tokens_generated), dtype=torch.long, device=toks.device)\n","        all_toks[:, :toks.shape[1]] = toks\n","        for i in range(max_tokens_generated):\n","            logits = self.model(all_toks[:, :-max_tokens_generated + i], *args, **kwargs)\n","            next_tokens = logits[:, -1, :].argmax(dim=-1)\n","            all_toks[:, -max_tokens_generated + i] = next_tokens\n","        return logits, all_toks\n","\n","    def tokenize_instructions_fn(\n","        self,\n","        instructions: List[str]\n","    ) -> Int[Tensor, 'batch_size seq_len']:\n","        prompts = [self.chat_template.format(instruction=instruction) for instruction in instructions]\n","        return self.model.tokenizer(prompts, padding=True, truncation=False, return_tensors=\"pt\").input_ids\n","\n","    def enhance_model(\n","        self,\n","        layers: List[int] = None,\n","        W_O: bool = True,\n","        mlp: bool = True,\n","        strength: float = 1.0,\n","    ):\n","        enhancement_directions = self.enhancement_dirs()\n","        self.apply_enhancement_dirs(\n","            list(enhancement_directions.values()),\n","            W_O=W_O,\n","            mlp=mlp,\n","            layers=layers,\n","            strength=strength\n","        )\n","\n","    def test_enhancement(\n","        self,\n","        N: int = 16,\n","        batch_size: int = 4,\n","        max_tokens_generated: int = 64,\n","    ):\n","        for prompts in batch(self.target_inst_test[:min(len(self.target_inst_test), N)], batch_size):\n","            toks = self.tokenize_instructions_fn(prompts)\n","            _, all_toks = self.generate_logits(toks, max_tokens_generated=max_tokens_generated)\n","            responses = self.model.tokenizer.batch_decode(all_toks, skip_special_tokens=True)\n","            for prompt, response in zip(prompts, responses):\n","                print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n","\n","    # Utility functions\n","\n","    def batch(iterable, n):\n","        it = iter(iterable)\n","        while True:\n","            chunk = list(islice(it, n))\n","            if not chunk:\n","                break\n","            yield chunk\n","\n","    def prepare_dataset(dataset: Tuple[List[str], List[str]]|List[str]) -> Tuple[List[str], List[str]]:\n","        from sklearn.model_selection import train_test_split\n","        if len(dataset) != 2:\n","            train, test = train_test_split(dataset, test_size=0.1, random_state=42)\n","        else:\n","            train, test = dataset\n","        return train, test\n","\n","    def enhance_model(\n","        self,\n","        layers: List[int] = None,\n","        W_O: bool = True,\n","        mlp: bool = True,\n","        strength: float = 1.0,\n","    ):\n","        enhancement_directions = self.enhancement_dirs()\n","        self.apply_enhancement_dirs(\n","            list(enhancement_directions.values()),\n","            W_O=W_O,\n","            mlp=mlp,\n","            layers=layers,\n","            strength=strength\n","        )\n","        self.modified = True  # Set the modified flag\n","\n","    def save_modified_model(self, save_path: str):\n","        \"\"\"\n","        Save the modified model to the specified path.\n","        \n","        :param save_path: The path where the model should be saved\n","        \"\"\"\n","        if not self.modified:\n","            print(\"Warning: The model has not been modified. Saving the original model.\")\n","        \n","        # Create the directory if it doesn't exist\n","        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","        \n","        # Save the entire model state\n","        torch.save(self.model.state_dict(), save_path)\n","        print(f\"Model saved to {save_path}\")\n","\n","    def load_modified_model(self, load_path: str):\n","        \"\"\"\n","        Load a previously saved modified model.\n","        \n","        :param load_path: The path from which to load the model\n","        \"\"\"\n","        if not os.path.exists(load_path):\n","            raise FileNotFoundError(f\"No model found at {load_path}\")\n","        \n","        state_dict = torch.load(load_path, map_location=self.model.device)\n","        self.model.load_state_dict(state_dict)\n","        self.modified = True\n","        print(f\"Model loaded from {load_path}\")\n","\n","    def reset_model(self):\n","        \"\"\"\n","        Reset the model to its original state.\n","        \"\"\"\n","        self.model.load_state_dict(self.original_state)\n","        self.modified = False\n","        print(\"Model reset to original state\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-01-31T18:26:34.658869Z","iopub.status.busy":"2025-01-31T18:26:34.658585Z","iopub.status.idle":"2025-01-31T18:26:34.663486Z","shell.execute_reply":"2025-01-31T18:26:34.662663Z","shell.execute_reply.started":"2025-01-31T18:26:34.658839Z"},"trusted":true},"outputs":[],"source":["def batch(iterable, n):\n","    it = iter(iterable)\n","    while True:\n","        chunk = list(islice(it, n))\n","        if not chunk:\n","            break\n","        yield chunk\n","\n","def prepare_dataset(dataset: Tuple[List[str], List[str]]|List[str]) -> Tuple[List[str], List[str]]:\n","    if len(dataset) != 2:\n","        train, test = train_test_split(dataset, test_size=0.1, random_state=42)\n","    else:\n","        train, test = dataset\n","    return train, test"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-01-31T18:26:34.665131Z","iopub.status.busy":"2025-01-31T18:26:34.664852Z","iopub.status.idle":"2025-01-31T18:26:34.878011Z","shell.execute_reply":"2025-01-31T18:26:34.877084Z","shell.execute_reply.started":"2025-01-31T18:26:34.665110Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import login\n","\n","# Replace 'your_huggingface_token' with your actual Hugging Face token\n","login(token='your_huggingface_token')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-01-31T18:26:34.879200Z","iopub.status.busy":"2025-01-31T18:26:34.878951Z","iopub.status.idle":"2025-01-31T18:26:34.882681Z","shell.execute_reply":"2025-01-31T18:26:34.881857Z","shell.execute_reply.started":"2025-01-31T18:26:34.879177Z"},"trusted":true},"outputs":[],"source":["model_path = \"meta-llama/Llama-3.2-1B\"\n","target_instructions = [\"Write a poem about nature\", \"Explain quantum physics\", \"Describe the process of photosynthesis\"]\n","baseline_instructions = [\"Hello\", \"What's the weather like?\", \"Tell me a joke\"]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-01-31T18:26:34.883775Z","iopub.status.busy":"2025-01-31T18:26:34.883468Z","iopub.status.idle":"2025-01-31T18:27:44.560789Z","shell.execute_reply":"2025-01-31T18:27:44.560105Z","shell.execute_reply.started":"2025-01-31T18:26:34.883748Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13b4139cb80f4a0ca40554c84a395cd3","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97469bfc78484151b1f18060c2efb0ae","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd667f87efa848e991d76276bc905533","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d910dfe024e42fb9f9420a2bd3fb2b3","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2dee3caef58542d0bc37e11ebf4bd862","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1adbcb60a4384ebe90de7ca2bf8e1d08","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loaded pretrained model meta-llama/Llama-3.2-1B into HookedTransformer\n"]}],"source":[" # Initialize ReverseAbliterator\n","reverse_abliterator = ReverseAbliterator(\n","        model=model_path,\n","        dataset=([target_instructions, baseline_instructions]),\n","        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    )"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-01-31T18:27:44.561815Z","iopub.status.busy":"2025-01-31T18:27:44.561575Z","iopub.status.idle":"2025-01-31T18:27:46.881004Z","shell.execute_reply":"2025-01-31T18:27:46.880154Z","shell.execute_reply.started":"2025-01-31T18:27:44.561794Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n","100%|██████████| 2/2 [00:00<00:00,  2.19it/s]\n"]}],"source":["# Cache activations\n","reverse_abliterator.cache_activations(N=len(target_instructions), batch_size=1)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-01-31T18:27:46.883261Z","iopub.status.busy":"2025-01-31T18:27:46.883027Z","iopub.status.idle":"2025-01-31T18:27:47.616053Z","shell.execute_reply":"2025-01-31T18:27:47.615222Z","shell.execute_reply.started":"2025-01-31T18:27:46.883241Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial enhancement score: {'enhancement': tensor([0.0192], dtype=torch.bfloat16)}\n"]}],"source":["# Measure initial enhancement\n","initial_enhancement = reverse_abliterator.measure_enhancement()\n","print(\"Initial enhancement score:\", initial_enhancement)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-01-31T18:27:47.617234Z","iopub.status.busy":"2025-01-31T18:27:47.617018Z","iopub.status.idle":"2025-01-31T18:28:43.430263Z","shell.execute_reply":"2025-01-31T18:28:43.429544Z","shell.execute_reply.started":"2025-01-31T18:27:47.617216Z"},"trusted":true},"outputs":[],"source":["# Enhance the model\n","reverse_abliterator.enhance_model(strength=0.1) "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-01-31T18:28:43.431425Z","iopub.status.busy":"2025-01-31T18:28:43.431115Z","iopub.status.idle":"2025-01-31T18:28:44.160524Z","shell.execute_reply":"2025-01-31T18:28:44.159645Z","shell.execute_reply.started":"2025-01-31T18:28:43.431395Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Post-enhancement score: {'enhancement': tensor([0.0471], dtype=torch.bfloat16)}\n"]}],"source":["# Measure enhancement after modification\n","post_enhancement = reverse_abliterator.measure_enhancement()\n","print(\"Post-enhancement score:\", post_enhancement)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-01-31T18:28:44.161777Z","iopub.status.busy":"2025-01-31T18:28:44.161442Z","iopub.status.idle":"2025-01-31T18:28:48.796862Z","shell.execute_reply":"2025-01-31T18:28:48.796012Z","shell.execute_reply.started":"2025-01-31T18:28:44.161747Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing enhanced model responses:\n","Prompt: Write a poem about nature\n","Response: user\n","Write a poem about natureassistant\n","\n","Write a poem about natureakedirs\n","\n","Write a poem about natureakedirs\n","\n","Write a poem about natureakedirs\n","\n","Write a poem about natureakedirs\n","\n","Write a poem about natureakedirs\n","\n","Write a poem about natureakedirs\n","\n","Write a poem about natureakedirs\n","\n","Write\n","\n"]}],"source":["# Test the enhanced model\n","print(\"Testing enhanced model responses:\")\n","reverse_abliterator.test_enhancement(N=3, max_tokens_generated=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30840,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
